{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "torch.backends.cudnn.bencmark = True\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os,sys,cv2,random,datetime,time,math\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from net_s3fd import *\n",
    "from s3fd import *\n",
    "from bbox import *\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class CelebDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping images and target labels\n",
    "    Arguments:\n",
    "        A CSV file path\n",
    "        Path to image folder\n",
    "        Extension of images\n",
    "        PIL transforms\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, img_path, img_ext, transform=None):\n",
    "    \n",
    "        tmp_df = pd.read_csv(csv_path)\n",
    "        assert tmp_df['Image_Name'].apply(lambda x: os.path.isfile(img_path + x + img_ext)).all(), \\\n",
    "\"Some images referenced in the CSV file were not found\"\n",
    "        \n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.img_path = img_path\n",
    "        self.img_ext = img_ext\n",
    "        self.transform = transform\n",
    "\n",
    "        self.X_train = tmp_df['Image_Name']\n",
    "        self.y_train = self.mlb.fit_transform(tmp_df['Gender'].str.split()).astype(np.float32)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img = cv2.imread(self.img_path + self.X_train[index] + self.img_ext)\n",
    "        img = cv2.resize(img, (256,256))\n",
    "        img = img - np.array([104,117,123])\n",
    "        img = img.transpose(2, 0, 1)\n",
    "        \n",
    "        #img = img.reshape((1,)+img.shape)\n",
    "        img = torch.from_numpy(img).float()\n",
    "        #img = Variable(torch.from_numpy(img).float(),volatile=True)\n",
    "        \n",
    "        #if self.transform is not None:\n",
    "        #    img = self.transform(img)\n",
    "        \n",
    "        label = torch.from_numpy(self.y_train[index])\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = \"index.csv\"\n",
    "img_path = \"data/Celeb_Small_Dataset/\"\n",
    "img_ext = \".jpg\"\n",
    "dset = CelebDataset(train_data,img_path,img_ext,None)\n",
    "train_loader = DataLoader(dset,\n",
    "                          batch_size=1,\n",
    "                          shuffle=True,\n",
    "                          num_workers=1 # 1 for CUDA\n",
    "                         # pin_memory = True # CUDA only\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save(model, optimizer, loss, filename):\n",
    "    save_dict = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss.data[0]\n",
    "        }\n",
    "    torch.save(save_dict, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_classes, num_epochs = 100):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i,(img,label) in enumerate(train_loader):\n",
    "            img = img.view((1,)+img.shape[1:])\n",
    "            if use_cuda:\n",
    "                data, target = Variable(img.cuda()), Variable(torch.Tensor(label).cuda())\n",
    "            else:\n",
    "                data, target = Variable(img), Variable(torch.Tensor(label))\n",
    "            target = target.view(num_classes,1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            olist = model(data)\n",
    "            for p in range(4): olist[p*2+1] = F.softmax(olist[p*2+1], dim = 1)\n",
    "            genscorelist = []\n",
    "            for p in range(4):\n",
    "                ocls,ogen = olist[p*2].data.cpu(),olist[p*2+1].data.cpu()\n",
    "                FB,FC,FH,FW = ocls.size()\n",
    "                stride = 2**(p+3) #8,16,32,64 ??\n",
    "                anchor = stride*4\n",
    "                for Findex in range(FH*FW):\n",
    "                    windex,hindex = Findex%FW,Findex//FW\n",
    "                    axc,ayc = stride/2+windex*stride,stride/2+hindex*stride\n",
    "                    score = ocls[0,1,hindex,windex]\n",
    "                    if score<0.05:continue\n",
    "                    genscorelist.append(ogen)\n",
    "            \n",
    "            loss = 0\n",
    "            for p in genscorelist:\n",
    "                loss += criterion(Variable(p, requires_grad=True),target)\n",
    "            \n",
    "            if i%50==0:\n",
    "                print(\"Reached iteration \",i)\n",
    "                running_loss += loss.data[0]\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.data[0]\n",
    "            \n",
    "            \n",
    "        if epoch % 10 == 0:\n",
    "            save(model, optimizer, loss, 'faceRecog.saved.model')\n",
    "        print(running_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "myModel = s3fd(num_classes)\n",
    "loadedModel = torch.load('s3fd_convert.pth')\n",
    "newModel = myModel.state_dict()\n",
    "pretrained_dict = {k: v for k, v in loadedModel.items() if k in newModel}\n",
    "newModel.update(pretrained_dict)\n",
    "myModel.load_state_dict(newModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3fd(\n",
       "  (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc6): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3))\n",
       "  (fc7): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv6_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv6_2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (conv7_1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv7_2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (fc_1): Linear(in_features=2304, out_features=2, bias=True)\n",
       "  (conv3_3_norm): L2Norm(\n",
       "  )\n",
       "  (conv4_3_norm): L2Norm(\n",
       "  )\n",
       "  (conv5_3_norm): L2Norm(\n",
       "  )\n",
       "  (conv3_3_norm_mbox_conf): Conv2d(256, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_3_norm_mbox_loc): Conv2d(256, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_3_norm_mbox_conf): Conv2d(512, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_3_norm_mbox_loc): Conv2d(512, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_3_norm_mbox_conf): Conv2d(512, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_3_norm_mbox_loc): Conv2d(512, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc7_mbox_conf): Conv2d(1024, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc7_mbox_loc): Conv2d(1024, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv6_2_mbox_conf): Conv2d(512, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv6_2_mbox_loc): Conv2d(512, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv7_2_mbox_conf): Conv2d(256, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv7_2_mbox_loc): Conv2d(256, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_3_norm_gender): Conv2d(512, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_3_norm_gender): Conv2d(512, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc7_norm_gender): Conv2d(1024, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv6_2_norm_gender): Conv2d(512, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc4_3): Linear(in_features=2048, out_features=2, bias=True)\n",
       "  (fc5_3): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (ffc7): Linear(in_features=288, out_features=2, bias=True)\n",
       "  (fc6_2): Linear(in_features=72, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = False\n",
    "myModel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "for param in myModel.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "#extract weights to a new tensor and add dimension\n",
    "hold_conv4_3 = myModel.conv4_3_norm_mbox_conf.weight[0].data.unsqueeze(0)\n",
    "hold_conv5_3 = myModel.conv5_3_norm_mbox_conf.weight[0].data.unsqueeze(0)\n",
    "hold_fc7 = myModel.fc7_mbox_conf.weight[0].data.unsqueeze(0)\n",
    "hold_conv6_2 = myModel.conv6_2_mbox_conf.weight[0].data.unsqueeze(0)\n",
    "\n",
    "#concatenate tensor and add to layer's weight\n",
    "myModel.conv4_3_norm_gender.weight = torch.nn.Parameter(torch.cat((hold_conv4_3, hold_conv4_3),0))\n",
    "myModel.conv5_3_norm_gender.weight = torch.nn.Parameter(torch.cat((hold_conv5_3, hold_conv5_3),0))\n",
    "myModel.fc7_norm_gender.weight = torch.nn.Parameter(torch.cat((hold_fc7, hold_fc7), 0))\n",
    "myModel.conv6_2_norm_gender.weight = torch.nn.Parameter(torch.cat((hold_conv6_2, hold_conv6_2), 0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 3, 3])\n",
      "torch.Size([2, 512, 3, 3])\n",
      "torch.Size([2, 1024, 3, 3])\n",
      "torch.Size([2, 512, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(myModel.conv4_3_norm_gender.weight.size())\n",
    "print(myModel.conv5_3_norm_gender.weight.size())\n",
    "print(myModel.fc7_norm_gender.weight.size())\n",
    "print(myModel.conv6_2_norm_gender.weight.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "Reached iteration  0\n",
      "Reached iteration  50\n",
      "Reached iteration  100\n",
      "Reached iteration  150\n",
      "663.4350594098214\n",
      "Epoch 1/99\n",
      "----------\n",
      "Reached iteration  0\n",
      "Reached iteration  50\n",
      "Reached iteration  100\n",
      "Reached iteration  150\n",
      "656.4354548903648\n",
      "Epoch 2/99\n",
      "----------\n",
      "Reached iteration  0\n",
      "Reached iteration  50\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad,myModel.parameters()), lr=0.0001, momentum=0.9)\n",
    "if use_cuda:\n",
    "    myModel = myModel.cuda()\n",
    "model_ft = train_model(myModel, criterion, optimizer, num_classes, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform(img_path):\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, (256,256))\n",
    "        img = img - np.array([104,117,123])\n",
    "        img = img.transpose(2, 0, 1)\n",
    "        \n",
    "        img = img.reshape((1,)+img.shape)\n",
    "        img = torch.from_numpy(img).float()\n",
    "        \n",
    "        return Variable(img.cuda())\n",
    "myModel = myModel.cuda()\n",
    "testImage1 = transform('data/Test/TestCeleb_4/25-FaceId-0.jpg')\n",
    "testImage2 = transform('data/Test/TestCeleb_4/26-FaceId-0.jpg')\n",
    "testImage3 = transform('data/Test/TestCeleb_4/27-FaceId-0.jpg')\n",
    "testImage4 = transform('data/Test/TestCeleb_10/25-FaceId-0.jpg')\n",
    "testImage5 = transform('data/Test/TestCeleb_10/26-FaceId-0.jpg')\n",
    "testImage6 = transform('data/Test/TestCeleb_10/24-FaceId-0.jpg')\n",
    "\n",
    "output1 = myModel(testImage1)\n",
    "output2 = myModel(testImage2)\n",
    "output3 = myModel(testImage2)\n",
    "output4 = myModel(testImage4)\n",
    "output5 = myModel(testImage5)\n",
    "output6 = myModel(testImage6)\n",
    "print(\"testImage1 - \",output1)\n",
    "print(\"testImage2 - \",output2)\n",
    "print(\"testImage3 - \",output3)\n",
    "print(\"testImage1 - \",output4)\n",
    "print(\"testImage2 - \",output5)\n",
    "print(\"testImage3 - \",output6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
